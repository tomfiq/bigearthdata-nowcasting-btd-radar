{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Evaluation Notebook (Test Split)\n\nNotebook ringkas untuk evaluasi model pada `test_leadXX.csv`.\n\nAnda hanya perlu mengubah path dataset dan path weights."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0. (Opsional) Install dependency"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "!pip -q install -r requirements.txt || true",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Konfigurasi path"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import os, sys, json\nfrom pathlib import Path\n\nREPO_DIR = Path('.').resolve()\nsys.path.insert(0, str(REPO_DIR))\n\n# ====== UBAH SESUAI LOKASI DATASET ANDA ======\nBASE_DIR = '/kaggle/input/NAMA_DATASET/NPZ'\nMANIFEST_DIR = '/kaggle/input/NAMA_DATASET/manifests'\nOUT_DIR = '/kaggle/working/run_h60_BTDRadar'\nos.makedirs(OUT_DIR, exist_ok=True)\n\n# ====== KONFIGURASI EVAL ======\nMODE = 'BTDRadar'\nHORIZON = 60\nBATCH_SIZE = 4\nWINDOW_STEPS = 12\nCADENCE_MIN = 10\nEXPECTED_HW = (128, 128)\n\n# weights: gunakan best.weights.h5 atau last.weights.h5\nWEIGHTS = os.path.join(OUT_DIR, 'best.weights.h5')\n\nprint('WEIGHTS:', WEIGHTS)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Import modul + siapkan test generator"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import pandas as pd\nimport tensorflow as tf\n\nfrom src.data.npz_sequence_tf import NPZSequence, NormStats\nfrom src.modeling import build_model, compile_model\nfrom src.metrics_tf import F1ScorePerChannel, ThreatScorePerChannel, BiasMetricPerChannel\n\ntest_csv = os.path.join(MANIFEST_DIR, f'test_lead{HORIZON}.csv')\n\n# Load norm jika tersedia\nnorm = None\nnorm_path = os.path.join(OUT_DIR, 'norm_stats.json')\nif os.path.exists(norm_path):\n    norm = NormStats.from_json(json.load(open(norm_path, 'r')))\n\ntest_seq = NPZSequence(\n    base_dir=BASE_DIR,\n    manifest_csv=test_csv,\n    mode=MODE,\n    horizon_min=HORIZON,\n    batch_size=BATCH_SIZE,\n    window_steps=WINDOW_STEPS,\n    cadence_min=CADENCE_MIN,\n    shuffle=False,\n    expected_hw=EXPECTED_HW,\n    flatten_time_channels=True,\n    norm_stats=norm,\n)\n\nprint('test batches:', len(test_seq))\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Build model, compile, load weights"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "alpha = None\nalpha_path = os.path.join(OUT_DIR, 'alpha.json')\nif os.path.exists(alpha_path):\n    alpha = json.load(open(alpha_path, 'r')).get('alpha')\nprint('alpha:', alpha)\n\nH, W = EXPECTED_HW\ninput_channels = WINDOW_STEPS * test_seq.c_base\n\nmodel = build_model(input_size=(H, W, input_channels), t_steps=WINDOW_STEPS, with_deep_supervision=True)\nmodel, _ = compile_model(\n    model,\n    y_train=tf.constant([0.0]),\n    steps_per_epoch=1,\n    total_epochs=1,\n    alpha_override=alpha,\n    extra_metrics=[\n        F1ScorePerChannel(0, name='f1_T1'),\n        F1ScorePerChannel(1, name='f1_T2'),\n        F1ScorePerChannel(2, name='f1_T3'),\n        ThreatScorePerChannel(0, name='ts_T1'),\n        ThreatScorePerChannel(1, name='ts_T2'),\n        ThreatScorePerChannel(2, name='ts_T3'),\n        BiasMetricPerChannel(0, name='bias_T1'),\n        BiasMetricPerChannel(1, name='bias_T2'),\n        BiasMetricPerChannel(2, name='bias_T3'),\n    ],\n)\n\nmodel.load_weights(WEIGHTS)\nprint('Loaded weights OK')\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Evaluate + simpan hasil ke JSON/CSV"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "results = model.evaluate(test_seq, verbose=1, return_dict=True)\nprint(results)\n\nout_json = os.path.join(OUT_DIR, f'eval_test_lead{HORIZON}.json')\nout_csv  = os.path.join(OUT_DIR, f'eval_test_lead{HORIZON}.csv')\njson.dump(results, open(out_json, 'w'), indent=2)\npd.DataFrame([results]).to_csv(out_csv, index=False)\nprint('Saved:', out_json)\nprint('Saved:', out_csv)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. (Opsional) Plot learning curve dari train_log.csv"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import matplotlib.pyplot as plt\n\nlog_path = os.path.join(OUT_DIR, 'train_log.csv')\nif os.path.exists(log_path):\n    df = pd.read_csv(log_path)\n    cols = [c for c in df.columns if ('pr_auc_T3' in c) or ('loss' in c)]\n    print('available cols:', cols)\n    if 'val_main_pr_auc_T3' in df.columns:\n        plt.figure()\n        plt.plot(df['val_main_pr_auc_T3'])\n        plt.title('val_main_pr_auc_T3 per epoch')\n        plt.xlabel('epoch')\n        plt.ylabel('PR-AUC')\n        plt.show()\nelse:\n    print('train_log.csv not found, skip plot')\n",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}