{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Training Notebook (Kaggle/Local)\n\nNotebook ringkas untuk training model **BTD + Radar \u2192 T1/T2/T3** menggunakan manifest CSV.\n\n**Anda hanya perlu mengubah path dataset** pada bagian `BASE_DIR` dan `MANIFEST_DIR`."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0. (Opsional) Install dependency\n\nJika Anda menjalankan di Kaggle dan belum menyiapkan environment sendiri, jalankan cell berikut. Jika sudah, boleh skip."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "!pip -q install -r requirements.txt || true",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Konfigurasi path\n\n- `BASE_DIR` menunjuk ke folder `NPZ/` yang berisi `btd_*/radar/t1/t2/t3`.\n- `MANIFEST_DIR` menunjuk ke folder `manifests/` yang berisi `train_leadXX.csv`, `val_leadXX.csv`, dll.\n- `OUT_DIR` adalah output training (weights/log)."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import os, sys, json\nfrom pathlib import Path\n\n# Jika notebook ini ada di root repo, REPO_DIR = '.'\nREPO_DIR = Path('.').resolve()\nsys.path.insert(0, str(REPO_DIR))  # agar `import src...` bekerja\n\n# ====== UBAH BAGIAN INI SESUAI LOKASI DATASET ANDA ======\n# Kaggle contoh:\n# BASE_DIR     = '/kaggle/input/NAMA_DATASET/NPZ'\n# MANIFEST_DIR = '/kaggle/input/NAMA_DATASET/manifests'\n# Local contoh:\n# BASE_DIR     = r'D:\\NPZ'\n# MANIFEST_DIR = r'D:\\NPZ\\manifests'\n\nBASE_DIR = '/kaggle/input/NAMA_DATASET/NPZ'\nMANIFEST_DIR = '/kaggle/input/NAMA_DATASET/manifests'\nOUT_DIR = '/kaggle/working/run_h60_BTDRadar'\nos.makedirs(OUT_DIR, exist_ok=True)\n\n# ====== KONFIGURASI EKSPERIMEN ======\nMODE = 'BTDRadar'      # 'BTDRadar' | 'onlyRadar' | 'BTDOnly'\nHORIZON = 60           # 10 | 30 | 60\nEPOCHS = 80\nBATCH_SIZE = 4\nWINDOW_STEPS = 12\nCADENCE_MIN = 10\nSEED = 42\nEXPECTED_HW = (128, 128)  # ubah jika berbeda\n\nprint('REPO_DIR:', REPO_DIR)\nprint('BASE_DIR:', BASE_DIR)\nprint('MANIFEST_DIR:', MANIFEST_DIR)\nprint('OUT_DIR:', OUT_DIR)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Import modul dan siapkan generator"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import tensorflow as tf\n\nfrom src.data.npz_sequence_tf import NPZSequence, compute_norm_stats, estimate_alpha_from_targets, NormStats\nfrom src.modeling import build_model, compile_model\nfrom src.metrics_tf import F1ScorePerChannel, ThreatScorePerChannel, BiasMetricPerChannel, LRSnapshot\n\ntf.keras.utils.set_random_seed(SEED)\n\ntrain_csv = os.path.join(MANIFEST_DIR, f'train_lead{HORIZON}.csv')\nval_csv   = os.path.join(MANIFEST_DIR, f'val_lead{HORIZON}.csv')\n\ntrain_seq = NPZSequence(\n    base_dir=BASE_DIR,\n    manifest_csv=train_csv,\n    mode=MODE,\n    horizon_min=HORIZON,\n    batch_size=BATCH_SIZE,\n    window_steps=WINDOW_STEPS,\n    cadence_min=CADENCE_MIN,\n    shuffle=True,\n    seed=SEED,\n    expected_hw=EXPECTED_HW,\n    flatten_time_channels=True,\n    norm_stats=None,\n)\n\nval_seq = NPZSequence(\n    base_dir=BASE_DIR,\n    manifest_csv=val_csv,\n    mode=MODE,\n    horizon_min=HORIZON,\n    batch_size=BATCH_SIZE,\n    window_steps=WINDOW_STEPS,\n    cadence_min=CADENCE_MIN,\n    shuffle=False,\n    seed=SEED,\n    expected_hw=EXPECTED_HW,\n    flatten_time_channels=True,\n    norm_stats=None,\n)\n\nprint('train batches:', len(train_seq), 'val batches:', len(val_seq))\nprint('C_base:', train_seq.c_base, 'input_channels:', WINDOW_STEPS * train_seq.c_base)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Hitung / load normalisasi (mean/std) dan alpha class-balance\n\nFile akan disimpan ke `OUT_DIR/norm_stats.json` dan `OUT_DIR/alpha.json`."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "norm_path = os.path.join(OUT_DIR, 'norm_stats.json')\nif os.path.exists(norm_path):\n    norm = NormStats.from_json(json.load(open(norm_path, 'r')))\nelse:\n    norm = compute_norm_stats(train_seq, max_batches=None)\n    json.dump(norm.to_json(), open(norm_path, 'w'), indent=2)\n\ntrain_seq.norm_stats = norm\nval_seq.norm_stats = norm\n\nalpha = estimate_alpha_from_targets(train_seq, max_batches=None)\njson.dump({'alpha': alpha}, open(os.path.join(OUT_DIR, 'alpha.json'), 'w'), indent=2)\n\nprint('norm mean:', norm.mean, 'norm std:', norm.std)\nprint('alpha:', alpha)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Build & compile model\n\n`input_size=(H,W,WINDOW_STEPS*C_base)` mengikuti format `(B,H,W,T*C)` yang digunakan temporal stem 3D."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "H, W = EXPECTED_HW\ninput_channels = WINDOW_STEPS * train_seq.c_base\n\nmodel = build_model(input_size=(H, W, input_channels), t_steps=WINDOW_STEPS, with_deep_supervision=True)\nmodel, _ = compile_model(\n    model,\n    y_train=tf.constant([0.0]),  # dummy, karena alpha_override dipakai\n    steps_per_epoch=len(train_seq),\n    total_epochs=EPOCHS,\n    base_lr=6e-4,\n    weight_decay=1e-4,\n    clip_norm=1.0,\n    alpha_override=alpha,\n    extra_metrics=[\n        F1ScorePerChannel(0, name='f1_T1'),\n        F1ScorePerChannel(1, name='f1_T2'),\n        F1ScorePerChannel(2, name='f1_T3'),\n        ThreatScorePerChannel(0, name='ts_T1'),\n        ThreatScorePerChannel(1, name='ts_T2'),\n        ThreatScorePerChannel(2, name='ts_T3'),\n        BiasMetricPerChannel(0, name='bias_T1'),\n        BiasMetricPerChannel(1, name='bias_T2'),\n        BiasMetricPerChannel(2, name='bias_T3'),\n    ],\n)\n\nmodel.summary()\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Training\n\n- `best.weights.h5`: bobot terbaik (monitor `val_main_pr_auc_T3`)\n- `last.weights.h5`: bobot terakhir\n- `train_log.csv`: log per epoch\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "callbacks = []\ncallbacks.append(LRSnapshot())\ncallbacks.append(tf.keras.callbacks.CSVLogger(os.path.join(OUT_DIR, 'train_log.csv')))\ncallbacks.append(tf.keras.callbacks.ModelCheckpoint(\n    filepath=os.path.join(OUT_DIR, 'best.weights.h5'),\n    monitor='val_main_pr_auc_T3',\n    mode='max',\n    save_best_only=True,\n    save_weights_only=True,\n    verbose=1,\n))\ncallbacks.append(tf.keras.callbacks.EarlyStopping(\n    monitor='val_main_pr_auc_T3',\n    mode='max',\n    patience=20,\n    restore_best_weights=True,\n    verbose=1,\n))\ncallbacks.append(tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_main_pr_auc_T3',\n    mode='max',\n    factor=0.5,\n    patience=5,\n    min_lr=1e-6,\n    verbose=1,\n))\n\nrun_cfg = {\n    'MODE': MODE,\n    'HORIZON': HORIZON,\n    'EPOCHS': EPOCHS,\n    'BATCH_SIZE': BATCH_SIZE,\n    'WINDOW_STEPS': WINDOW_STEPS,\n    'CADENCE_MIN': CADENCE_MIN,\n    'EXPECTED_HW': EXPECTED_HW,\n    'SEED': SEED,\n}\njson.dump(run_cfg, open(os.path.join(OUT_DIR, 'run_config.json'), 'w'), indent=2)\n\nhistory = model.fit(\n    train_seq,\n    validation_data=val_seq,\n    epochs=EPOCHS,\n    callbacks=callbacks,\n    verbose=1,\n    workers=2,\n    use_multiprocessing=True,\n)\n\nmodel.save_weights(os.path.join(OUT_DIR, 'last.weights.h5'))\njson.dump(history.history, open(os.path.join(OUT_DIR, 'history.json'), 'w'), indent=2)\nprint('Saved outputs to:', OUT_DIR)\n",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}